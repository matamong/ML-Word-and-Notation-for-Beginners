# **ML Words And Notation For Beginners**

머신러닝을 처음 접할 때 만나는 무섭고 어려운 용어와 표기법들 때문에 다음과 같은 상황이 온다!😱 <br>
- *기존 공부의 흐름이 끊겨버림*
- *단어가 무엇이었는지 헷갈림*
- *검색해도 어려운 말이나 수식밖에 없음*

이것들을 대비하여 도움이 될 수 있도록 용어와 표기법을 간단하게만 정리하고 이해하고 공부하는 Repo! 🙌 <br>
(틀렸거나 / 추가하고싶거나 / 더 쉽게 설명할 수 있는 / 내용이 있다면 누구나 `PR` 이나 `Issue` 주세요!)
<br>

👉 **[용어항목으로](#🆎용어)** <br>
👉 **[표기법항목으로](#✍️표기법)** <br>

<br><br>



## **🆎용어**

- **Bias(편향)**
    - **`Neural Network(신경망)` 에서의 `Bias`** : 
        - 하나의 노드로 입력된 모든 값을 더한 뒤 그 값에 더해주는 **상수** 로, 노드의 활성화 함수를 거쳐 **최종적으로 출력되는 값을 조절하는(도와주는) 역할** 을 한다.  즉, 이 `상수` 를 더해줌으로써 활성화 함수의 그래프 적합도를 높이고, 모든 입력이 0인 경우에도 노드가 활성화되는 것을 보장하게 하여 편리함을 취하는 것이다. 상수는 랜덤으로 초기화되며 트레이닝을 통해 최적값을 가지게된다.

<br>

- **Cost Function(비용함수)**
    - 훈련세트들이 얼마나 잘 추측되었는지 측정해주는 함수. 
    - 일반적으로 `Cost Function(비용함수)` = `Loss Function(손실함수)` = `Object Function(목적 함수)` 모두 같은 말이라고 보는 편이다.

<br>

- **Loss Function(손실함수)**
    - 단일 훈련 셋에서 손실이 얼마나 발생했는지 측정해주는 함수. 
    - 일반적으로 `Loss Function(비용함수)` = `Cost Function(손실함수)` = `Object Function(목적 함수)` 모두 같은 말이라고 보는 편이다.

- **Maximum Likelihood Estimation(최대가능도)**

<br>

- **Normalization(정규화)**
    - 머신러닝에서, 데이터가 가진 Feature의 범위가 너무 심하게 차이가 나서 영향력이 클 때 혼란스럽지않도록 범위를 0과 1 사이의 값으로 바꾸는 것.

<br>

- **Residual(잔차)** 
    - **회귀식으로부터 얻은 예측값과 실제 관측값의 차이**를 뜻한다. 회귀식으로부터 그어진 일차방정식 직선과 실제 데이터의 수직 거리를 활용하여 잔차를 구한다. `Linear Regression(선형 회귀)`에서 이 `residual` 을 사용하여 `least squares(최소제곱)`방법을 이용한다. 당연하게도 이 `residual`이 적을수록 좋다.
        - **Error(오차)** - `Residual` 과 `Error` 는 다르다.
            - `Error` -  `Population(모집단)` 으로부터 추정한 회귀식과 실제 관측값의 차이.
            - `Residual` - `Sample(표본집단)` 로부터 추정한 회귀식과 실제 관측값의 차이.

<br>

- **Vector(벡터)**
    


- **Vectorization(벡터화)**
    - 일종의 병렬 처리로써 방대한 양의 데이터를 효율적으로 처리할 수 있게한다. 데이터의 양이 방대한 머신러닝에선 느리고 비효율적인 `Loop` 문을 사용하지않는 대신에 주로 이 `Vecotrization` 을 이용한다.

<br>

- **Weight(가중치)** 
    - 인공 신경망 모형의 하나인 퍼셉트론 알고리즘에서 나온 것으로 **뉴런과 뉴런간의 신호 혹은 연결의 세기(결과에 얼마나 영향을 미치는지)** 를 말한다. 이 `weight` 는 처음에는 랜덤하게 초기화되어있지만 점점 트레이닝 할수록 최적값을 가지게된다.(그래야만..한다....)
        - 처음 입력데이터가 주어지면 뉴런1은 입력받은 데이터에 랜덤하게 초기화된 `weight` 값을 곱한다.(결과에 끼치는 영향력을 조절한다.) 결과적으로 처음 입력된 데이터와는 다른 데이터를 뉴런2에게 건내주게된다.
        - 뉴런2는 마찬가지로 입력받은 데이터에 `weight` 를 곱하여 처음 입력받은 데이터와는 다른 데이터를 다음 뉴런에게 보낸다. 
        - 이를 반복하여 학습함으로써 최적의 `weight` 가 드러나게된다.

        이처럼 `weight`는 입력데이터를 바꿈으로써 결과값을 조절할 수 있기 때문에 **입력 데이터를 변환하는 신경망 내의 `Parameter(매게변수)`** 로 본다.


<br><br>

## **✍️표기법**

- **e (자연상수e)** 
    - 𝝅(파이)가 3.14이듯이 `e` 는 2.71828을 나타내는 상수이다. 자연 성장에 대한 의미를 가지고 있어서 기울기등을 찾는 미적분학에 잘 어울린다. `Logistic function(로지스틱 함수)` 등에서 볼 수 있다.

<br>

- **ŷ (y_hat)** 
    - 예측결과값을 의미한다.

<br>

- **t** 
    - **`전치행렬 t(transposed matrix) `**
        - 행과 열을 교환하여 얻는 행렬. 머신러닝 계산을 할 때, 차원을 일치시키기위해 전치행렬을 자주 쓴다.
        - 전치행렬 과정 
        - ![https://ko.wikipedia.org/wiki/%EC%A0%84%EC%B9%98%ED%96%89%EB%A0%AC](https://upload.wikimedia.org/wikipedia/commons/e/e4/Matrix_transpose.gif)
